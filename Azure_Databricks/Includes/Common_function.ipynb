{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80344bd9-1d9e-4c63-af7c-000c48fe9185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc15eebe-826f-44f7-8635-1bc9e96ac3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_column_value(df, column_name, old_value1, new_value1,old_value2, new_value2):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes categorical values in a given column.\n",
    "\n",
    "    - Applies TRIM to remove leading/trailing spaces.\n",
    "    - Applies UPPER to make the comparison case-insensitive.\n",
    "    - Replaces matching values:\n",
    "        * old_value1 → new_value1\n",
    "        * old_value2 → new_value2\n",
    "    - All other values are replaced with 'n/a'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        Input DataFrame.\n",
    "    column_name : str\n",
    "        Column to normalize.\n",
    "    old_value1 : str\n",
    "        Original value (after trim/upper) to be mapped.\n",
    "    new_value1 : str\n",
    "        Replacement value for old_value1.\n",
    "    old_value2 : str\n",
    "        Original value (after trim/upper) to be mapped.\n",
    "    new_value2 : str\n",
    "        Replacement value for old_value2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        DataFrame with the normalized column.\n",
    "    \"\"\"\n",
    "    df_new = df.withColumn(column_name,F.when(F.upper(F.trim(F.col(column_name)))==old_value1,new_value1)\\\n",
    "        .when(F.upper(F.trim(F.col(column_name)))==old_value2,new_value2)\n",
    "        .otherwise('n/a'))\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0ac07a-cf97-4fde-b474-2b6185e841c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_access_data_lake():\n",
    "    client_id= dbutils.secrets.get(scope=\"salesdwhscope\", key=\"clientid\")\n",
    "    tenant_id=dbutils.secrets.get(scope=\"salesdwhscope\", key=\"tenantid\")\n",
    "    client_secret=dbutils.secrets.get(scope=\"salesdwhscope\", key=\"clientsecret\")\n",
    "    spark.conf.set(\"fs.azure.account.auth.type.salesdwh.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(\"fs.azure.account.oauth.provider.type.salesdwh.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.id.salesdwh.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.secret.salesdwh.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.salesdwh.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c9382a-0409-46fc-8afa-0b41ac28820a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "def icremental_load(df_input,catalog_name,schema_name,table_name,merge_condition):\n",
    "    \"\"\"\n",
    "    Perform an incremental load (Upsert) into a Delta Lake table.\n",
    "\n",
    "    This function checks if a Delta table already exists in the given \n",
    "    catalog and schema:\n",
    "      - If the table exists → it performs a MERGE operation between \n",
    "        the target table and the input DataFrame (`df_input`) using \n",
    "        the provided `merge_condition`.\n",
    "          * Matching rows are updated (Update All).\n",
    "          * Non-matching rows are inserted (Insert All).\n",
    "      - If the table does not exist → it creates a new Delta table \n",
    "        and writes the input DataFrame as the initial load.\n",
    "\n",
    "    Spark dynamic partition pruning is enabled for optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input : pyspark.sql.DataFrame\n",
    "        The source DataFrame containing new or updated records.\n",
    "\n",
    "    catalog_name : str\n",
    "        The Unity Catalog name (or hive_metastore if not using UC).\n",
    "\n",
    "    schema_name : str\n",
    "        The schema (database) name where the Delta table resides.\n",
    "\n",
    "    table_name : str\n",
    "        The target Delta table name.\n",
    "\n",
    "    merge_condition : str\n",
    "        SQL-style merge condition string (e.g. \"tgt.id = src.id\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function writes/updates the Delta table in-place.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (spark.catalog.tableExists(f\"{catalog_name}.{schema_name}.{table_name}\")):\n",
    "        deltaTable = DeltaTable.forName(spark, f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "        merge_result = (deltaTable.alias(\"tgt\").merge(df_input.alias(\"src\"), merge_condition)\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute())\n",
    "\n",
    "        df_pandas = merge_result.toPandas()\n",
    "        affected =df_pandas['num_affected_rows'].iloc[0]\n",
    "        updated = df_pandas['num_updated_rows'].iloc[0]\n",
    "        inserted = df_pandas['num_inserted_rows'].iloc[0]\n",
    "        if updated == 0 and inserted == 0 and affected == 0 :\n",
    "            message =f\" No changes applied :[The affected rows = {affected}]-----[The updated rows = {updated}]----[The inserted rows = {inserted}] \"\n",
    "        else:\n",
    "            message = f\" Merge completed: [The affected rows = {affected}]-----[The updated rows = {updated}]----[The inserted rows ={inserted}] \"\n",
    "        return message\n",
    "\n",
    "    else:\n",
    "        df_input.write.format(\"delta\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "        message = f\" Table {catalog_name}.{schema_name}.{table_name}  is created the first one.\"\n",
    "        return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1e4468-64ba-4d76-830f-537134be31cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_date_column(df, col_name):\n",
    "    \"\"\"\n",
    "    Cleans a date column in a Spark DataFrame.\n",
    "\n",
    "    Logic:\n",
    "    - If the value = 0 OR the string length of the value is not 8 → set it to NULL.\n",
    "    - Otherwise, cast the value to string and parse it as a date in format 'yyyyMMdd'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The input DataFrame.\n",
    "    col_name : str\n",
    "        The name of the column to clean and convert.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        A new DataFrame with the specified column cleaned and converted to a proper DateType.\n",
    "    \"\"\"\n",
    "    return df.withColumn(col_name,F.when((F.col(col_name) == 0) | (F.length(F.col(col_name).cast(\"string\")) != 8),None)\\\n",
    "        .otherwise(F.to_date(F.col(col_name).cast(\"string\"), \"yyyyMMdd\")))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}